{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T S. Vishnu [1], A. Bhattacharya [2]\n",
    "#### 1 - Junior Undergrad, 2 - Senior Undergrad\n",
    "### Indian Institute of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_f = open('imdb-data/reviews.txt','r')\n",
    "reviews = text_f.read()\n",
    "labels_f = open('imdb-data/labels.txt','r')\n",
    "labels = labels_f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "positive\n",
      "negativ\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:1000])\n",
    "print(labels[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.lower()\n",
    "text = ''.join([c for c in reviews if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_split = text.split('\\n')\n",
    "text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n",
      "25001\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_split))\n",
    "labels = labels.split(\"\\n\")\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life \n",
      "same\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])\n",
    "print(words[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Build a dictionary that maps words to integers \n",
    "counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n",
      "6020196\n"
     ]
    }
   ],
   "source": [
    "print(len(counts))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(encoded_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_reviews = []\n",
    "[encoded_reviews.append([vocab_to_int[word] for word in r.split()]) for r in reviews_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the encoded reviews length is the same as a normal review\n",
    "#[print(len(reviews_split[i].split()), len(encoded_reviews[i])) for i in range(len(reviews_split))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels])\n",
    "print(labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in encoded_reviews])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "# get indices of any reviews with length 0 \n",
    "non_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(review) != 0]\n",
    "# remove 0-length reviews and their labels \n",
    "reviews_ints = [encoded_reviews[ii] for ii in non_zero_idx] \n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])  \n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 21025   308     6     3  1050   207     8  2138    32     1   171    57\n",
      "    15    49    81  5785    44   382   110   140    15  5194    60   154\n",
      "     9     1  4975  5852   475    71     5   260    12 21025   308    13\n",
      "  1978     6    74  2395     5   613    73     6  5194     1 24103     5\n",
      "  1983 10166     1  5786  1499    36    51    66   204   145    67  1199\n",
      "  5194 19869     1 37442     4     1   221   883    31  2988    71     4\n",
      "     1  5787    10   686     2    67  1499    54    10   216     1   383\n",
      "     9    62     3  1406  3686   783     5  3483   180     1   382    10\n",
      "  1212 13583    32   308     3   349   341  2913    10   143   127     5\n",
      "  7690    30     4   129  5194  1406  2326     5 21025   308    10   528\n",
      "    12   109  1448     4    60   543   102    12 21025   308     6   227\n",
      "  4146    48     3  2211    12     8   215    23] 1\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0    63     4     3   125    36    47  7472  1395    16     3\n",
      "  4181   505    45    17     3   622   134    12     6     3  1279   457\n",
      "     4  1721   207     3 10624  7373   300     6   667    83    35  2116\n",
      "  1086  2989    34     1   898 46417     4     8    13  5096   464     8\n",
      "  2656  1721     1   221    57    17    58   794  1297   832   228     8\n",
      "    43    98   123  1469    59   147    38     1   963   142    29   667\n",
      "   123     1 13584   410    61    94  1774   306   755     5     3   819\n",
      " 10396    22     3  1724   635     8    13   128    73    21   233   102\n",
      "    17    49    50   617    34   682    85 28785 28786   682   374  3341\n",
      " 11398     2 16371  7946    51    29   108  3324] 0\n",
      "[22382    42 46418    15   706 17139  3389    47    77    35  1819    16\n",
      "   154    19   114     3  1305     5   336   147    22     1   857    12\n",
      "    70   281  1168   399    36   120   283    38   169     5   382   158\n",
      "    42  2269    16     1   541    90    78   102     4     1  3244    15\n",
      "    43     3   407  1068   136  8055    44   182   140    15  3043     1\n",
      "   320    22  4818 26224   346     5  3090  2092     1 18839 17939    42\n",
      "  8055    46    33   236    29   370     5   130    56    22     1  1928\n",
      "     7     7    19    48    46    21    70   344     3  2099     5   408\n",
      "    22     1  1928    16     3  3119   205     1 28787    21   281    68\n",
      "    38     3   339     1   700   715     3  3818  1229    22     1  1491\n",
      "     3  1197     2   283    21   281  2435     5    66    48     8    13\n",
      "    39     5    29  3244    12     6 21026 11723    13  2015     7     7\n",
      "  3687  2818    36  4147    36   374    15 11723   296     3   996   125\n",
      "    36    47   283     9     1   176   363  6893     5    94     3  2099\n",
      "    17     3  4976  2932 14557 19870     5    66    46    25    51   408\n",
      "     9     1  1928    16  3236   490   205     1 28787    46 11723  2845\n",
      "    25    51    80    48    25   483    17     3] 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(features[i][:200], encoded_labels[i]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x), len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 2500\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 2500\n"
     ]
    }
   ],
   "source": [
    "print(len(val_x), len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ...,   52,  724,   18],\n",
      "        [  30,    4,    1,  ..., 1374,   20,   34],\n",
      "        [   0,    0,    0,  ...,   12,   16, 1544],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,   23,   29,  672],\n",
      "        [ 988, 2035, 4214,  ...,    3,   74, 6914],\n",
      "        [   0,    0,    0,  ...,    7,    1, 7819]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentRNN(74072, 1, 200, 128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2214.126296148\n",
      "10\n",
      "Epoch: 1/4... Step: 10... Loss: 0.552676... Val Loss: 0.566647\n",
      "20\n",
      "Epoch: 1/4... Step: 20... Loss: 0.478660... Val Loss: 0.584911\n",
      "30\n",
      "Epoch: 1/4... Step: 30... Loss: 0.535999... Val Loss: 0.575509\n",
      "40\n",
      "Epoch: 1/4... Step: 40... Loss: 0.431518... Val Loss: 0.572853\n",
      "50\n",
      "Epoch: 1/4... Step: 50... Loss: 0.654851... Val Loss: 0.561881\n",
      "60\n",
      "Epoch: 1/4... Step: 60... Loss: 0.443313... Val Loss: 0.602107\n",
      "70\n",
      "Epoch: 1/4... Step: 70... Loss: 0.619176... Val Loss: 0.572406\n",
      "80\n",
      "Epoch: 1/4... Step: 80... Loss: 0.504361... Val Loss: 0.552677\n",
      "90\n",
      "Epoch: 1/4... Step: 90... Loss: 0.450999... Val Loss: 0.546876\n",
      "100\n",
      "Epoch: 1/4... Step: 100... Loss: 0.785503... Val Loss: 0.585338\n",
      "110\n",
      "Epoch: 1/4... Step: 110... Loss: 0.584925... Val Loss: 0.533098\n",
      "120\n",
      "Epoch: 1/4... Step: 120... Loss: 0.539047... Val Loss: 0.577684\n",
      "130\n",
      "Epoch: 1/4... Step: 130... Loss: 0.512821... Val Loss: 0.581343\n",
      "140\n",
      "Epoch: 1/4... Step: 140... Loss: 0.529345... Val Loss: 0.585661\n",
      "150\n",
      "Epoch: 1/4... Step: 150... Loss: 0.389597... Val Loss: 0.638944\n",
      "160\n",
      "Epoch: 1/4... Step: 160... Loss: 0.541751... Val Loss: 0.588581\n",
      "170\n",
      "Epoch: 1/4... Step: 170... Loss: 0.510743... Val Loss: 0.561932\n",
      "180\n",
      "Epoch: 1/4... Step: 180... Loss: 0.550291... Val Loss: 0.585242\n",
      "190\n",
      "Epoch: 1/4... Step: 190... Loss: 0.513548... Val Loss: 0.579153\n",
      "200\n",
      "Epoch: 1/4... Step: 200... Loss: 0.521596... Val Loss: 0.575063\n",
      "210\n",
      "Epoch: 1/4... Step: 210... Loss: 0.379274... Val Loss: 0.537063\n",
      "220\n",
      "Epoch: 1/4... Step: 220... Loss: 0.594596... Val Loss: 0.534528\n",
      "230\n",
      "Epoch: 1/4... Step: 230... Loss: 0.553429... Val Loss: 0.519122\n",
      "240\n",
      "Epoch: 1/4... Step: 240... Loss: 0.389530... Val Loss: 0.507842\n",
      "250\n",
      "Epoch: 1/4... Step: 250... Loss: 0.503811... Val Loss: 0.561485\n",
      "260\n",
      "Epoch: 1/4... Step: 260... Loss: 0.450736... Val Loss: 0.529469\n",
      "270\n",
      "Epoch: 1/4... Step: 270... Loss: 0.532762... Val Loss: 0.521102\n",
      "280\n",
      "Epoch: 1/4... Step: 280... Loss: 0.582827... Val Loss: 0.528828\n",
      "290\n",
      "Epoch: 1/4... Step: 290... Loss: 0.551319... Val Loss: 0.571945\n",
      "300\n",
      "Epoch: 1/4... Step: 300... Loss: 0.563648... Val Loss: 0.550539\n",
      "310\n",
      "Epoch: 1/4... Step: 310... Loss: 0.610404... Val Loss: 0.556398\n",
      "320\n",
      "Epoch: 1/4... Step: 320... Loss: 0.608084... Val Loss: 0.561679\n",
      "330\n",
      "Epoch: 1/4... Step: 330... Loss: 0.521460... Val Loss: 0.646393\n",
      "340\n",
      "Epoch: 1/4... Step: 340... Loss: 0.631122... Val Loss: 0.505011\n",
      "350\n",
      "Epoch: 1/4... Step: 350... Loss: 0.444835... Val Loss: 0.526974\n",
      "360\n",
      "Epoch: 1/4... Step: 360... Loss: 0.447331... Val Loss: 0.515332\n",
      "370\n",
      "Epoch: 1/4... Step: 370... Loss: 0.369290... Val Loss: 0.507283\n",
      "380\n",
      "Epoch: 1/4... Step: 380... Loss: 0.373229... Val Loss: 0.511079\n",
      "390\n",
      "Epoch: 1/4... Step: 390... Loss: 0.448864... Val Loss: 0.495866\n",
      "400\n",
      "Epoch: 1/4... Step: 400... Loss: 0.470324... Val Loss: 0.559073\n",
      "2911.711362442\n",
      "410\n",
      "Epoch: 2/4... Step: 410... Loss: 0.554413... Val Loss: 0.677257\n",
      "420\n",
      "Epoch: 2/4... Step: 420... Loss: 0.574312... Val Loss: 0.515317\n",
      "430\n",
      "Epoch: 2/4... Step: 430... Loss: 0.357280... Val Loss: 0.537849\n",
      "440\n",
      "Epoch: 2/4... Step: 440... Loss: 0.358202... Val Loss: 0.503619\n",
      "450\n",
      "Epoch: 2/4... Step: 450... Loss: 0.550420... Val Loss: 0.616920\n",
      "460\n",
      "Epoch: 2/4... Step: 460... Loss: 0.420529... Val Loss: 0.542632\n",
      "470\n",
      "Epoch: 2/4... Step: 470... Loss: 0.402886... Val Loss: 0.508896\n",
      "480\n",
      "Epoch: 2/4... Step: 480... Loss: 0.709615... Val Loss: 0.586321\n",
      "490\n",
      "Epoch: 2/4... Step: 490... Loss: 0.521060... Val Loss: 0.518234\n",
      "500\n",
      "Epoch: 2/4... Step: 500... Loss: 0.325805... Val Loss: 0.511072\n",
      "510\n",
      "Epoch: 2/4... Step: 510... Loss: 0.342317... Val Loss: 0.528261\n",
      "520\n",
      "Epoch: 2/4... Step: 520... Loss: 0.306589... Val Loss: 0.522553\n",
      "530\n",
      "Epoch: 2/4... Step: 530... Loss: 0.206555... Val Loss: 0.520542\n",
      "540\n",
      "Epoch: 2/4... Step: 540... Loss: 0.339505... Val Loss: 0.532451\n",
      "550\n",
      "Epoch: 2/4... Step: 550... Loss: 0.444401... Val Loss: 0.496056\n",
      "560\n",
      "Epoch: 2/4... Step: 560... Loss: 0.526137... Val Loss: 0.661081\n",
      "570\n",
      "Epoch: 2/4... Step: 570... Loss: 0.431950... Val Loss: 0.515665\n",
      "580\n",
      "Epoch: 2/4... Step: 580... Loss: 0.356250... Val Loss: 0.528559\n",
      "590\n",
      "Epoch: 2/4... Step: 590... Loss: 0.494107... Val Loss: 0.519376\n",
      "600\n",
      "Epoch: 2/4... Step: 600... Loss: 0.317080... Val Loss: 0.549096\n",
      "610\n",
      "Epoch: 2/4... Step: 610... Loss: 0.520134... Val Loss: 0.515503\n",
      "620\n",
      "Epoch: 2/4... Step: 620... Loss: 0.425620... Val Loss: 0.546606\n",
      "630\n",
      "Epoch: 2/4... Step: 630... Loss: 0.344897... Val Loss: 0.511258\n",
      "640\n",
      "Epoch: 2/4... Step: 640... Loss: 0.440848... Val Loss: 0.509560\n",
      "650\n",
      "Epoch: 2/4... Step: 650... Loss: 0.420423... Val Loss: 0.524287\n",
      "660\n",
      "Epoch: 2/4... Step: 660... Loss: 0.444265... Val Loss: 0.553919\n",
      "670\n",
      "Epoch: 2/4... Step: 670... Loss: 0.329480... Val Loss: 0.524513\n",
      "680\n",
      "Epoch: 2/4... Step: 680... Loss: 0.375875... Val Loss: 0.509272\n",
      "690\n",
      "Epoch: 2/4... Step: 690... Loss: 0.360086... Val Loss: 0.516440\n",
      "700\n",
      "Epoch: 2/4... Step: 700... Loss: 0.283165... Val Loss: 0.544308\n",
      "710\n",
      "Epoch: 2/4... Step: 710... Loss: 0.375766... Val Loss: 0.483757\n",
      "720\n",
      "Epoch: 2/4... Step: 720... Loss: 0.507325... Val Loss: 0.492159\n",
      "730\n",
      "Epoch: 2/4... Step: 730... Loss: 0.349225... Val Loss: 0.483182\n",
      "740\n",
      "Epoch: 2/4... Step: 740... Loss: 0.276760... Val Loss: 0.486386\n",
      "750\n",
      "Epoch: 2/4... Step: 750... Loss: 0.266384... Val Loss: 0.478004\n",
      "760\n",
      "Epoch: 2/4... Step: 760... Loss: 0.338334... Val Loss: 0.486205\n",
      "770\n",
      "Epoch: 2/4... Step: 770... Loss: 0.208862... Val Loss: 0.502412\n",
      "780\n",
      "Epoch: 2/4... Step: 780... Loss: 0.462772... Val Loss: 0.506861\n",
      "790\n",
      "Epoch: 2/4... Step: 790... Loss: 0.444317... Val Loss: 0.464699\n",
      "800\n",
      "Epoch: 2/4... Step: 800... Loss: 0.413384... Val Loss: 0.482966\n",
      "3590.297070762\n",
      "810\n",
      "Epoch: 3/4... Step: 810... Loss: 0.303706... Val Loss: 0.499771\n",
      "820\n",
      "Epoch: 3/4... Step: 820... Loss: 0.267308... Val Loss: 0.511097\n",
      "830\n",
      "Epoch: 3/4... Step: 830... Loss: 0.232856... Val Loss: 0.554562\n",
      "840\n",
      "Epoch: 3/4... Step: 840... Loss: 0.244826... Val Loss: 0.481044\n",
      "850\n",
      "Epoch: 3/4... Step: 850... Loss: 0.444103... Val Loss: 0.500220\n",
      "860\n",
      "Epoch: 3/4... Step: 860... Loss: 0.417091... Val Loss: 0.607878\n",
      "870\n",
      "Epoch: 3/4... Step: 870... Loss: 0.283858... Val Loss: 0.480349\n",
      "880\n",
      "Epoch: 3/4... Step: 880... Loss: 0.446721... Val Loss: 0.477413\n",
      "890\n",
      "Epoch: 3/4... Step: 890... Loss: 0.361137... Val Loss: 0.495178\n",
      "900\n",
      "Epoch: 3/4... Step: 900... Loss: 0.259715... Val Loss: 0.487210\n",
      "910\n",
      "Epoch: 3/4... Step: 910... Loss: 0.331892... Val Loss: 0.507046\n",
      "920\n",
      "Epoch: 3/4... Step: 920... Loss: 0.287168... Val Loss: 0.488783\n",
      "930\n",
      "Epoch: 3/4... Step: 930... Loss: 0.299547... Val Loss: 0.527471\n",
      "940\n",
      "Epoch: 3/4... Step: 940... Loss: 0.408165... Val Loss: 0.475788\n",
      "950\n",
      "Epoch: 3/4... Step: 950... Loss: 0.235745... Val Loss: 0.475189\n",
      "960\n",
      "Epoch: 3/4... Step: 960... Loss: 0.220423... Val Loss: 0.497184\n",
      "970\n",
      "Epoch: 3/4... Step: 970... Loss: 0.257124... Val Loss: 0.501429\n",
      "980\n",
      "Epoch: 3/4... Step: 980... Loss: 0.178740... Val Loss: 0.628151\n",
      "990\n",
      "Epoch: 3/4... Step: 990... Loss: 0.373759... Val Loss: 0.586806\n",
      "1000\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.299249... Val Loss: 0.513901\n",
      "1010\n",
      "Epoch: 3/4... Step: 1010... Loss: 0.460662... Val Loss: 0.491081\n",
      "1020\n",
      "Epoch: 3/4... Step: 1020... Loss: 0.453013... Val Loss: 0.561156\n",
      "1030\n",
      "Epoch: 3/4... Step: 1030... Loss: 0.323657... Val Loss: 0.496977\n",
      "1040\n",
      "Epoch: 3/4... Step: 1040... Loss: 0.239064... Val Loss: 0.501966\n",
      "1050\n",
      "Epoch: 3/4... Step: 1050... Loss: 0.226465... Val Loss: 0.524257\n",
      "1060\n",
      "Epoch: 3/4... Step: 1060... Loss: 0.374869... Val Loss: 0.537982\n",
      "1070\n",
      "Epoch: 3/4... Step: 1070... Loss: 0.250732... Val Loss: 0.499701\n",
      "1080\n",
      "Epoch: 3/4... Step: 1080... Loss: 0.254124... Val Loss: 0.523937\n",
      "1090\n",
      "Epoch: 3/4... Step: 1090... Loss: 0.269275... Val Loss: 0.548364\n",
      "1100\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.344346... Val Loss: 0.494853\n",
      "1110\n",
      "Epoch: 3/4... Step: 1110... Loss: 0.221054... Val Loss: 0.487698\n",
      "1120\n",
      "Epoch: 3/4... Step: 1120... Loss: 0.365706... Val Loss: 0.512080\n",
      "1130\n",
      "Epoch: 3/4... Step: 1130... Loss: 0.287012... Val Loss: 0.474193\n",
      "1140\n",
      "Epoch: 3/4... Step: 1140... Loss: 0.365659... Val Loss: 0.554063\n",
      "1150\n",
      "Epoch: 3/4... Step: 1150... Loss: 0.275905... Val Loss: 0.515376\n",
      "1160\n",
      "Epoch: 3/4... Step: 1160... Loss: 0.184146... Val Loss: 0.565433\n",
      "1170\n",
      "Epoch: 3/4... Step: 1170... Loss: 0.284278... Val Loss: 0.544422\n",
      "1180\n",
      "Epoch: 3/4... Step: 1180... Loss: 0.369494... Val Loss: 0.542843\n",
      "1190\n",
      "Epoch: 3/4... Step: 1190... Loss: 0.338487... Val Loss: 0.504303\n",
      "1200\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.269683... Val Loss: 0.474643\n",
      "4234.994851221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210\n",
      "Epoch: 4/4... Step: 1210... Loss: 0.307710... Val Loss: 0.538702\n",
      "1220\n",
      "Epoch: 4/4... Step: 1220... Loss: 0.238222... Val Loss: 0.500253\n",
      "1230\n",
      "Epoch: 4/4... Step: 1230... Loss: 0.298452... Val Loss: 0.536204\n",
      "1240\n",
      "Epoch: 4/4... Step: 1240... Loss: 0.176908... Val Loss: 0.520016\n",
      "1250\n",
      "Epoch: 4/4... Step: 1250... Loss: 0.144472... Val Loss: 0.507130\n",
      "1260\n",
      "Epoch: 4/4... Step: 1260... Loss: 0.295805... Val Loss: 0.560244\n",
      "1270\n",
      "Epoch: 4/4... Step: 1270... Loss: 0.305527... Val Loss: 0.499520\n",
      "1280\n",
      "Epoch: 4/4... Step: 1280... Loss: 0.165900... Val Loss: 0.525920\n",
      "1290\n",
      "Epoch: 4/4... Step: 1290... Loss: 0.178712... Val Loss: 0.530073\n",
      "1300\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.170125... Val Loss: 0.501474\n",
      "1310\n",
      "Epoch: 4/4... Step: 1310... Loss: 0.218819... Val Loss: 0.507185\n",
      "1320\n",
      "Epoch: 4/4... Step: 1320... Loss: 0.229444... Val Loss: 0.547995\n",
      "1330\n",
      "Epoch: 4/4... Step: 1330... Loss: 0.397476... Val Loss: 0.542285\n",
      "1340\n",
      "Epoch: 4/4... Step: 1340... Loss: 0.140559... Val Loss: 0.493368\n",
      "1350\n",
      "Epoch: 4/4... Step: 1350... Loss: 0.282182... Val Loss: 0.532825\n",
      "1360\n",
      "Epoch: 4/4... Step: 1360... Loss: 0.286872... Val Loss: 0.584470\n",
      "1370\n",
      "Epoch: 4/4... Step: 1370... Loss: 0.226555... Val Loss: 0.513011\n",
      "1380\n",
      "Epoch: 4/4... Step: 1380... Loss: 0.301441... Val Loss: 0.499811\n",
      "1390\n",
      "Epoch: 4/4... Step: 1390... Loss: 0.148867... Val Loss: 0.528801\n",
      "1400\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.271920... Val Loss: 0.526511\n",
      "1410\n",
      "Epoch: 4/4... Step: 1410... Loss: 0.281518... Val Loss: 0.511145\n",
      "1420\n",
      "Epoch: 4/4... Step: 1420... Loss: 0.407820... Val Loss: 0.526047\n",
      "1430\n",
      "Epoch: 4/4... Step: 1430... Loss: 0.268563... Val Loss: 0.542021\n",
      "1440\n",
      "Epoch: 4/4... Step: 1440... Loss: 0.239197... Val Loss: 0.534120\n",
      "1450\n",
      "Epoch: 4/4... Step: 1450... Loss: 0.131211... Val Loss: 0.593705\n",
      "1460\n",
      "Epoch: 4/4... Step: 1460... Loss: 0.216647... Val Loss: 0.534687\n",
      "1470\n",
      "Epoch: 4/4... Step: 1470... Loss: 0.185682... Val Loss: 0.533110\n",
      "1480\n",
      "Epoch: 4/4... Step: 1480... Loss: 0.284577... Val Loss: 0.530117\n",
      "1490\n",
      "Epoch: 4/4... Step: 1490... Loss: 0.267236... Val Loss: 0.555748\n",
      "1500\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.219037... Val Loss: 0.513533\n",
      "1510\n",
      "Epoch: 4/4... Step: 1510... Loss: 0.201548... Val Loss: 0.549851\n",
      "1520\n",
      "Epoch: 4/4... Step: 1520... Loss: 0.157946... Val Loss: 0.570943\n",
      "1530\n",
      "Epoch: 4/4... Step: 1530... Loss: 0.145364... Val Loss: 0.536028\n",
      "1540\n",
      "Epoch: 4/4... Step: 1540... Loss: 0.335005... Val Loss: 0.511458\n",
      "1550\n",
      "Epoch: 4/4... Step: 1550... Loss: 0.178490... Val Loss: 0.495592\n",
      "1560\n",
      "Epoch: 4/4... Step: 1560... Loss: 0.414789... Val Loss: 0.502824\n",
      "1570\n",
      "Epoch: 4/4... Step: 1570... Loss: 0.403811... Val Loss: 0.528018\n",
      "1580\n",
      "Epoch: 4/4... Step: 1580... Loss: 0.236119... Val Loss: 0.513401\n",
      "1590\n",
      "Epoch: 4/4... Step: 1590... Loss: 0.376879... Val Loss: 0.534784\n",
      "1600\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.202419... Val Loss: 0.503397\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "    \n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    print(timeit.default_timer())\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        #print(counter)\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            print(counter)\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.173\n",
      "Train accuracy: 0.936\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in train_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    #if(train_on_gpu):\n",
    "        #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Train loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(train_loader.dataset)\n",
    "print(\"Train accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
